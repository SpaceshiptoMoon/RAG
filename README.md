'# Python异步爬虫教程\n\n## 1. 异步爬虫简介\n\n异步爬虫利用异步编程模型，能够同时处理多个网络请求，显著提高爬取效率。相比传统的同步爬虫，异步爬虫在I/O密集型任务中表现更优。\n\n## 2. 核心库介绍\n\n### 主要依赖库\n```python\nimport asyncio\nimport aiohttp\nfrom bs4 import BeautifulSoup\nimport time\nfrom typing import List, Dict, Optional\n```\n\n## 3. 基础异步爬虫示例\n\n### 简单的异步HTTP请求\n```python\nimport asyncio\nimport aiohttp\nimport time\n\nasync def fetch_url(session: aiohttp.ClientSession, url: str) -> str:\n    """异步获取网页内容"""\n    try:\n        async with session.get(url) as response:\n            return await response.text()\n    except Exception as e:\n        print(f"请求失败 {url}: {e}")\n        return ""\n\nasync def main():\n    urls = [\n        \'https://httpbin.org/delay/1\',\n        \'https://httpbin.org/delay/2\',\n        \'https://httpbin.org/delay/1\'\n    ]\n    \n    start_time = time.time()\n    \n    # 创建会话\n    async with aiohttp.ClientSession() as session:\n        # 并发执行所有请求\n        tasks = [fetch_url(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n        \n        print(f"完成 {len(results)} 个请求")\n        print(f"耗时: {time.time() - start_time:.2f}秒")\n\n# 运行异步程序\nif __name__ == "__main__":\n    asyncio.run(main())\n```\n\n## 4. 完整的异步爬虫框架\n\n### 配置类\n```python\nclass CrawlerConfig:\n    """爬虫配置类"""\n    def __init__(self):\n        self.max_concurrent_requests = 10  # 最大并发请求数\n        self.request_timeout = 10          # 请求超时时间(秒)\n        self.retry_times = 3         
      # 重试次数\n        self.delay_range = (0.5, 1.5)      # 请求间隔随机延迟范围\n        self.headers = {\n            \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\'\n        }\n        self.proxy = None                  # 代理设置\n        \n    def get_session_config(self):\n        """获取会话配置"""\n        return {\n            \'timeout\': aiohttp.ClientTimeout(total=self.request_timeout),\n            \'headers\': self.headers,\n            \'proxy\': self.proxy\n        }\n```\n\n### 异步爬虫核心类\n```python\nclass AsyncCrawler:\n    """异步爬虫核心类"""\n    \n    def __init__(self, config: CrawlerConfig):\n        self.config = config\n        self.session: Optional[aiohttp.ClientSession] = None\n        self.semaphore = asyncio.Semaphore(config.max_concurrent_requests)\n        \n    async def __aenter__(self):\n        """异步上下文管理器入口"""\n        self.session = aiohttp.ClientSession(**self.config.get_session_config())\n        return self\n        \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        """异步上下文管理器出口"""\n        if self.session:\n            await self.session.close()\n            \n    async def _fetch_with_retry(self, url: str) -> Optional[str]:\n        """带重试机制的请求"""\n        for attempt in range(self.config.retry_times):\n            try:\n                async with self.semaphore:  # 控制并发\n                    async with self.session.get(url) as response:\n                       
 if response.status == 200:\n                            content = await response.text()\n                            \n                  
          # 添加随机延迟\n                            delay = random.uniform(*self.config.delay_range)\n                            await asyncio.sleep(delay)\n                            \n                            return content\n                        else:\n           
                 print(f"状态码错误 {url}: {response.status}")\n                            \n            except Exception as e:\n                print(f"第{attempt + 1}次尝试失败 {url}: {e}")\n                \n            # 重试前等待\n            if attempt < self.config.retry_times - 1:\n                wait_time = 2 ** attempt  # 指数退避\n                await asyncio.sleep(wait_time)\n                \n        return None\n        \n    async def crawl_urls(self, urls: List[str]) -> List[Dict]:\n        """批量爬取URL列表"""\n        tasks = [self._fetch_single_url(url) for url in urls]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # 处理异常\n        valid_results = []\n        for result in results:\n            if isinstance(result, Exception):\n                print(f"请求异常: {result}")\n            elif result is not None:\n                valid_results.append(result)\n                \n        return valid_results\n        \n    async def _fetch_single_url(self, url: str) -> Optional[Dict]:\n        """爬取单个URL并解析"""\n        html = await self._fetch_with_retry(url)\n        if not html:\n            return None\n            \n        try:\n            soup = BeautifulSoup(html, \'html.parser\')\n            # 提取标题\n            title_tag = soup.find(\'title\')\n            title = title_tag.get_text().strip() if title_tag else "无标题"\n            \n            # 提取正文（示例）\n            paragraphs = soup.find_all(\'p\')\n            content = \' \'.join([p.get_text().strip() for p in paragraphs[:5]])\n            \n            return {\n                
\'url\': url,\n                \'title\': title,\n                \'content\': content[:200] + \'...\' if len(content) > 200 else content,\n                \'timestamp\': time.time()\n            }\n            \n        except Exception as e:\n            print(f"解析失败 {url}: {e}")\n            return None\n```\n\n## 5. 实际应用示例\n\n### 爬取多个页面\n```python\nimport random\nimport asyncio\nfrom typing import List\n\nasync def demo_crawler():\n    """演示爬虫使用"""\n    # 配置爬虫\n    config = CrawlerConfig()\n    config.max_concurrent_requests = 5\n    config.delay_range = (0.1, 0.5)\n    \n    # 测试URL列表\n    urls = [\n        \'https://httpbin.org/html\',\n        \'https://httpbin.org/json\',\n        \'https://httpbin.org/xml\',\n        \'https://httpbin.org/robots.txt\',\n        \'https://httpbin.org/user-agent\',\n        \'https://httpbin.org/headers\',\n        \'https://httpbin.org/ip\',\n        \'https://httpbin.org/uuid\'\n    ] * 2  # 重复以增加数量\n    \n    start_time = time.time()\n    \n    async with AsyncCrawler(config) as crawler:\n        results = await crawler.crawl_urls(urls)\n        \n        print(f"\\n=== 爬取完成 ===")\n        print(f"成功爬取: {len(results)}/{len(urls)}")\n        print(f"总耗时: {time.time() - start_time:.2f}秒")\n        \n        # 显示结果\n        for i, result in enumerate(results[:5]):  # 只显示前5个\n            print(f"{i+1}. {result[\'title\']} - {result[\'url\']}")\n            \n    return results\n\n# 运行演示\nif __name__ == "__main__":\n    asyncio.run(demo_crawler())\n```\n\n## 6. 高级功能实现\n\n### 分页爬取\n```python\nclass PagedCrawler(AsyncCrawler):\n    """分页爬虫类"""\n    \n    async def crawl_pages(self, base_url: str, page_range: range) -> List[Dict]:\n        """爬取分页内容"""\n        urls = [f"{base_url}?page={page}" for page in page_range]\n        return await self.crawl_urls(urls)\n        \n    async def dynamic_crawl(self, seed_url: str, max_pages: int = 100) -> List[Dict]:\n        """动态爬取（发现新链接）"""\n        visited = set()\n        to_visit = {seed_url}\n        results = []\n        \n        while to_visit and len(visited) < max_pages:\n            current_url = to_visit.pop()\n            if current_url in visited:\n                continue\n                \n            visited.add(current_url)\n            \n            # 获取页面内容\n            html = await self._fetch_with_retry(current_url)\n            if not html:\n                continue\n                \n            # 解析并提取新链接\n            soup = BeautifulSoup(html, \'html.parser\')\n            links = soup.find_all(\'a\', href=True)\n            \n            for link in links:\n                href = link[\'href\']\n                # 相对路径转绝对路径\n                full_url = urllib.parse.urljoin(seed_url, href)\n                if full_url.startswith(seed_url) and full_url not in visited:\n                    to_visit.add(full_url)\n                    \n            # 处理当前页面\n            result = await self._fetch_single_url(current_url)\n            if result:\n                results.append(result)\n                
\n        return results\n```\n\n### 数据存储\n```python\nimport json\nimport csv\nfrom datetime import datetime\n\nclass DataExporter:\n    """数据导出工具"""\n    \n    @staticmethod\n    def export_to_json(data: List[Dict], filename: str):\n        """导出为JSON文件"""\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        filepath = f"{filename}_{timestamp}.json"\n        \n        with open(filepath, \'w\', encoding=\'utf-8\') as f:\n            json.dump(data, f, ensure_ascii=False, indent=2)\n            \n        print(f" 数据已保存到: {filepath}")\n        \n    @staticmethod\n    def export_to_csv(data: List[Dict], filename: str):\n        """导出为CSV文件"""\n        if not data:\n            return\n            \n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        filepath = f"{filename}_{timestamp}.csv"\n        \n        keys = data[0].keys()\n        with open(filepath, \'w\', newline=\'\', encoding=\'utf-8\') as f:\n            writer = csv.DictWriter(f, fieldnames=keys)\n            writer.writeheader()\n            writer.writerows(data)\n            \n        print(f"数据已保存到: {filepath}")\n```\n\n## 7. 注意事项和最佳实践\n\n### ⚠️ 重要注意事项\n\n#### 1. 合法性和道  德性\n```python\n# 始终遵守robots.txt规则\nimport urllib.robotparser\n\ndef check_robots_txt(base_url: str, user_agent: str = \'*\') -> bool:\n    """检查robots.txt"""\n    rp = urllib.robotparser.RobotFileParser()\n    rp.set_url(f"{base_url}/robots.txt")\n    rp.read()\n    return rp.can_fetch(user_agent, base_url)\n```\n\n#### 2. 速率限制\n```python\n# 使用信号量控制并发\nsemaphore = asyncio.Semaphore(10)  # 限制最大并发数\n\n# 添加随机延迟\nawait asyncio.sleep(random.uniform(0.5, 1.5))\n```\n\n#### 3. 错误处理\n```python\nasync def safe_request(session, url):\n    try:\n        async with session.get(url) as response:\n            response.raise_for_status()  # 检查HTTP状态码\n            return await response.text()\n    except aiohttp.ClientError as e:\n        print(f"客户端错误: {e}")\n    except asyncio.TimeoutError:\n        print("请求超时")\n    except Exception as e:\n        print(f"未知错误: {e}")\n    return None\n```\n\n### 🛡️ 安全和 反爬措施\n\n#### 用户代理轮换\n```python\nUSER_AGENTS = [\n    \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\',\n    \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\',\n    \'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\'\n]\n\ndef get_random_user_agent():\n    return random.choice(USER_AGENTS)\n```\n\n#### 代理池\n```python\nclass ProxyPool:\n    """代理池管理"""\n    def __init__(self, proxies: List[str]):\n        self.proxies = proxies\n        self.current_index = 0\n        \n    def get_next_proxy(self) -> Optional[str]:\n        if not self.proxies:\n            return None\n        proxy = self.proxies[self.current_index]\n        self.current_index = (self.current_index + 1) % len(self.proxies)\n        return proxy\n```\n\n### 📊 性能优化建议\n\n1. **连接复用**：始终使用`ClientSession`\n2. **适当并发**：根据目标服务器承受能力调整\n3. **缓存机制**：避免重复请求相同内容\n4. **资源清理**：及时关闭会话和连接\n\n### 🔧 调试技巧\n\n```python\n# 启用详细日志\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# 监控性能\nstart = time.time()\n# ... 爬虫代码 ...\nprint(f"耗时: {time.time() - start:.2f}s")\n```\n\n## 8. 完整示例：新闻网站爬取\n\n```python\nasync def main_news_crawler():\n    """新闻网站爬取示例"""\n    config = CrawlerConfig()\n    config.max_concurrent_requests = 3\n    config.headers.update({\n        \'User-Agent\': \'Mozilla/5.0 (compatible; NewsBot/1.0)\'\n    })\n    \n    news_urls = [\n        \'https://example-news.com/article/1\',\n        \'https://example-news.com/article/2\',\n        # ... 更多URL\n    ]\n    \n    async with AsyncCrawler(config) as crawler:\n        results = await crawler.crawl_urls(news_urls)\n        \n        # 导出数据\n        exporter = DataExporter()\n        exporter.export_to_json(results, "news_data")\n        exporter.export_to_csv(results, "news_data")\n\n# 运行\nif __name__ == "__main__":\n    asyncio.run(main_news_crawler())\n```\n\n## 总结\n\n异步爬虫的优势：\n- ✅ 高效利用网络I/O\n- ✅ 显著提升爬取速度\n- ✅ 资源利用率高\n\n使用建议：\n- 遵守网站规则和法律法规\n- 合理设置请求频率\n- 做好错误处理和重试机制\n- 监控系统资源使用情况\n\n通过合理使用异步爬虫，可以高效地完成大规模数据采集任务，但务必注意合法合规使用。'