'# Pythonå¼‚æ­¥çˆ¬è™«æ•™ç¨‹\n\n## 1. å¼‚æ­¥çˆ¬è™«ç®€ä»‹\n\nå¼‚æ­¥çˆ¬è™«åˆ©ç”¨å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªç½‘ç»œè¯·æ±‚ï¼Œæ˜¾è‘—æé«˜çˆ¬å–æ•ˆç‡ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„åŒæ­¥çˆ¬è™«ï¼Œå¼‚æ­¥çˆ¬è™«åœ¨I/Oå¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¼˜ã€‚\n\n## 2. æ ¸å¿ƒåº“ä»‹ç»\n\n### ä¸»è¦ä¾èµ–åº“\n```python\nimport asyncio\nimport aiohttp\nfrom bs4 import BeautifulSoup\nimport time\nfrom typing import List, Dict, Optional\n```\n\n## 3. åŸºç¡€å¼‚æ­¥çˆ¬è™«ç¤ºä¾‹\n\n### ç®€å•çš„å¼‚æ­¥HTTPè¯·æ±‚\n```python\nimport asyncio\nimport aiohttp\nimport time\n\nasync def fetch_url(session: aiohttp.ClientSession, url: str) -> str:\n    """å¼‚æ­¥è·å–ç½‘é¡µå†…å®¹"""\n    try:\n        async with session.get(url) as response:\n            return await response.text()\n    except Exception as e:\n        print(f"è¯·æ±‚å¤±è´¥ {url}: {e}")\n        return ""\n\nasync def main():\n    urls = [\n        \'https://httpbin.org/delay/1\',\n        \'https://httpbin.org/delay/2\',\n        \'https://httpbin.org/delay/1\'\n    ]\n    \n    start_time = time.time()\n    \n    # åˆ›å»ºä¼šè¯\n    async with aiohttp.ClientSession() as session:\n        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚\n        tasks = [fetch_url(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n        \n        print(f"å®Œæˆ {len(results)} ä¸ªè¯·æ±‚")\n        print(f"è€—æ—¶: {time.time() - start_time:.2f}ç§’")\n\n# è¿è¡Œå¼‚æ­¥ç¨‹åº\nif __name__ == "__main__":\n    asyncio.run(main())\n```\n\n## 4. å®Œæ•´çš„å¼‚æ­¥çˆ¬è™«æ¡†æ¶\n\n### é…ç½®ç±»\n```python\nclass CrawlerConfig:\n    """çˆ¬è™«é…ç½®ç±»"""\n    def __init__(self):\n        self.max_concurrent_requests = 10  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°\n        self.request_timeout = 10          # è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)\n        self.retry_times = 3         
      # é‡è¯•æ¬¡æ•°\n        self.delay_range = (0.5, 1.5)      # è¯·æ±‚é—´éš”éšæœºå»¶è¿ŸèŒƒå›´\n        self.headers = {\n            \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\'\n        }\n        self.proxy = None                  # ä»£ç†è®¾ç½®\n        \n    def get_session_config(self):\n        """è·å–ä¼šè¯é…ç½®"""\n        return {\n            \'timeout\': aiohttp.ClientTimeout(total=self.request_timeout),\n            \'headers\': self.headers,\n            \'proxy\': self.proxy\n        }\n```\n\n### å¼‚æ­¥çˆ¬è™«æ ¸å¿ƒç±»\n```python\nclass AsyncCrawler:\n    """å¼‚æ­¥çˆ¬è™«æ ¸å¿ƒç±»"""\n    \n    def __init__(self, config: CrawlerConfig):\n        self.config = config\n        self.session: Optional[aiohttp.ClientSession] = None\n        self.semaphore = asyncio.Semaphore(config.max_concurrent_requests)\n        \n    async def __aenter__(self):\n        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""\n        self.session = aiohttp.ClientSession(**self.config.get_session_config())\n        return self\n        \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""\n        if self.session:\n            await self.session.close()\n            \n    async def _fetch_with_retry(self, url: str) -> Optional[str]:\n        """å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚"""\n        for attempt in range(self.config.retry_times):\n            try:\n                async with self.semaphore:  # æ§åˆ¶å¹¶å‘\n                    async with self.session.get(url) as response:\n                       
 if response.status == 200:\n                            content = await response.text()\n                            \n                  
          # æ·»åŠ éšæœºå»¶è¿Ÿ\n                            delay = random.uniform(*self.config.delay_range)\n                            await asyncio.sleep(delay)\n                            \n                            return content\n                        else:\n           
                 print(f"çŠ¶æ€ç é”™è¯¯ {url}: {response.status}")\n                            \n            except Exception as e:\n                print(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ {url}: {e}")\n                \n            # é‡è¯•å‰ç­‰å¾…\n            if attempt < self.config.retry_times - 1:\n                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿\n                await asyncio.sleep(wait_time)\n                \n        return None\n        \n    async def crawl_urls(self, urls: List[str]) -> List[Dict]:\n        """æ‰¹é‡çˆ¬å–URLåˆ—è¡¨"""\n        tasks = [self._fetch_single_url(url) for url in urls]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # å¤„ç†å¼‚å¸¸\n        valid_results = []\n        for result in results:\n            if isinstance(result, Exception):\n                print(f"è¯·æ±‚å¼‚å¸¸: {result}")\n            elif result is not None:\n                valid_results.append(result)\n                \n        return valid_results\n        \n    async def _fetch_single_url(self, url: str) -> Optional[Dict]:\n        """çˆ¬å–å•ä¸ªURLå¹¶è§£æ"""\n        html = await self._fetch_with_retry(url)\n        if not html:\n            return None\n            \n        try:\n            soup = BeautifulSoup(html, \'html.parser\')\n            # æå–æ ‡é¢˜\n            title_tag = soup.find(\'title\')\n            title = title_tag.get_text().strip() if title_tag else "æ— æ ‡é¢˜"\n            \n            # æå–æ­£æ–‡ï¼ˆç¤ºä¾‹ï¼‰\n            paragraphs = soup.find_all(\'p\')\n            content = \' \'.join([p.get_text().strip() for p in paragraphs[:5]])\n            \n            return {\n                
\'url\': url,\n                \'title\': title,\n                \'content\': content[:200] + \'...\' if len(content) > 200 else content,\n                \'timestamp\': time.time()\n            }\n            \n        except Exception as e:\n            print(f"è§£æå¤±è´¥ {url}: {e}")\n            return None\n```\n\n## 5. å®é™…åº”ç”¨ç¤ºä¾‹\n\n### çˆ¬å–å¤šä¸ªé¡µé¢\n```python\nimport random\nimport asyncio\nfrom typing import List\n\nasync def demo_crawler():\n    """æ¼”ç¤ºçˆ¬è™«ä½¿ç”¨"""\n    # é…ç½®çˆ¬è™«\n    config = CrawlerConfig()\n    config.max_concurrent_requests = 5\n    config.delay_range = (0.1, 0.5)\n    \n    # æµ‹è¯•URLåˆ—è¡¨\n    urls = [\n        \'https://httpbin.org/html\',\n        \'https://httpbin.org/json\',\n        \'https://httpbin.org/xml\',\n        \'https://httpbin.org/robots.txt\',\n        \'https://httpbin.org/user-agent\',\n        \'https://httpbin.org/headers\',\n        \'https://httpbin.org/ip\',\n        \'https://httpbin.org/uuid\'\n    ] * 2  # é‡å¤ä»¥å¢åŠ æ•°é‡\n    \n    start_time = time.time()\n    \n    async with AsyncCrawler(config) as crawler:\n        results = await crawler.crawl_urls(urls)\n        \n        print(f"\\n=== çˆ¬å–å®Œæˆ ===")\n        print(f"æˆåŠŸçˆ¬å–: {len(results)}/{len(urls)}")\n        print(f"æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")\n        \n        # æ˜¾ç¤ºç»“æœ\n        for i, result in enumerate(results[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª\n            print(f"{i+1}. {result[\'title\']} - {result[\'url\']}")\n            \n    return results\n\n# è¿è¡Œæ¼”ç¤º\nif __name__ == "__main__":\n    asyncio.run(demo_crawler())\n```\n\n## 6. é«˜çº§åŠŸèƒ½å®ç°\n\n### åˆ†é¡µçˆ¬å–\n```python\nclass PagedCrawler(AsyncCrawler):\n    """åˆ†é¡µçˆ¬è™«ç±»"""\n    \n    async def crawl_pages(self, base_url: str, page_range: range) -> List[Dict]:\n        """çˆ¬å–åˆ†é¡µå†…å®¹"""\n        urls = [f"{base_url}?page={page}" for page in page_range]\n        return await self.crawl_urls(urls)\n        \n    async def dynamic_crawl(self, seed_url: str, max_pages: int = 100) -> List[Dict]:\n        """åŠ¨æ€çˆ¬å–ï¼ˆå‘ç°æ–°é“¾æ¥ï¼‰"""\n        visited = set()\n        to_visit = {seed_url}\n        results = []\n        \n        while to_visit and len(visited) < max_pages:\n            current_url = to_visit.pop()\n            if current_url in visited:\n                continue\n                \n            visited.add(current_url)\n            \n            # è·å–é¡µé¢å†…å®¹\n            html = await self._fetch_with_retry(current_url)\n            if not html:\n                continue\n                \n            # è§£æå¹¶æå–æ–°é“¾æ¥\n            soup = BeautifulSoup(html, \'html.parser\')\n            links = soup.find_all(\'a\', href=True)\n            \n            for link in links:\n                href = link[\'href\']\n                # ç›¸å¯¹è·¯å¾„è½¬ç»å¯¹è·¯å¾„\n                full_url = urllib.parse.urljoin(seed_url, href)\n                if full_url.startswith(seed_url) and full_url not in visited:\n                    to_visit.add(full_url)\n                    \n            # å¤„ç†å½“å‰é¡µé¢\n            result = await self._fetch_single_url(current_url)\n            if result:\n                results.append(result)\n                
\n        return results\n```\n\n### æ•°æ®å­˜å‚¨\n```python\nimport json\nimport csv\nfrom datetime import datetime\n\nclass DataExporter:\n    """æ•°æ®å¯¼å‡ºå·¥å…·"""\n    \n    @staticmethod\n    def export_to_json(data: List[Dict], filename: str):\n        """å¯¼å‡ºä¸ºJSONæ–‡ä»¶"""\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        filepath = f"{filename}_{timestamp}.json"\n        \n        with open(filepath, \'w\', encoding=\'utf-8\') as f:\n            json.dump(data, f, ensure_ascii=False, indent=2)\n            \n        print(f" æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")\n        \n    @staticmethod\n    def export_to_csv(data: List[Dict], filename: str):\n        """å¯¼å‡ºä¸ºCSVæ–‡ä»¶"""\n        if not data:\n            return\n            \n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        filepath = f"{filename}_{timestamp}.csv"\n        \n        keys = data[0].keys()\n        with open(filepath, \'w\', newline=\'\', encoding=\'utf-8\') as f:\n            writer = csv.DictWriter(f, fieldnames=keys)\n            writer.writeheader()\n            writer.writerows(data)\n            \n        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")\n```\n\n## 7. æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®è·µ\n\n### âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\n\n#### 1. åˆæ³•æ€§å’Œé“  å¾·æ€§\n```python\n# å§‹ç»ˆéµå®ˆrobots.txtè§„åˆ™\nimport urllib.robotparser\n\ndef check_robots_txt(base_url: str, user_agent: str = \'*\') -> bool:\n    """æ£€æŸ¥robots.txt"""\n    rp = urllib.robotparser.RobotFileParser()\n    rp.set_url(f"{base_url}/robots.txt")\n    rp.read()\n    return rp.can_fetch(user_agent, base_url)\n```\n\n#### 2. é€Ÿç‡é™åˆ¶\n```python\n# ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘\nsemaphore = asyncio.Semaphore(10)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°\n\n# æ·»åŠ éšæœºå»¶è¿Ÿ\nawait asyncio.sleep(random.uniform(0.5, 1.5))\n```\n\n#### 3. é”™è¯¯å¤„ç†\n```python\nasync def safe_request(session, url):\n    try:\n        async with session.get(url) as response:\n            response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç \n            return await response.text()\n    except aiohttp.ClientError as e:\n        print(f"å®¢æˆ·ç«¯é”™è¯¯: {e}")\n    except asyncio.TimeoutError:\n        print("è¯·æ±‚è¶…æ—¶")\n    except Exception as e:\n        print(f"æœªçŸ¥é”™è¯¯: {e}")\n    return None\n```\n\n### ğŸ›¡ï¸ å®‰å…¨å’Œ åçˆ¬æªæ–½\n\n#### ç”¨æˆ·ä»£ç†è½®æ¢\n```python\nUSER_AGENTS = [\n    \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\',\n    \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\',\n    \'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\'\n]\n\ndef get_random_user_agent():\n    return random.choice(USER_AGENTS)\n```\n\n#### ä»£ç†æ± \n```python\nclass ProxyPool:\n    """ä»£ç†æ± ç®¡ç†"""\n    def __init__(self, proxies: List[str]):\n        self.proxies = proxies\n        self.current_index = 0\n        \n    def get_next_proxy(self) -> Optional[str]:\n        if not self.proxies:\n            return None\n        proxy = self.proxies[self.current_index]\n        self.current_index = (self.current_index + 1) % len(self.proxies)\n        return proxy\n```\n\n### ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®\n\n1. **è¿æ¥å¤ç”¨**ï¼šå§‹ç»ˆä½¿ç”¨`ClientSession`\n2. **é€‚å½“å¹¶å‘**ï¼šæ ¹æ®ç›®æ ‡æœåŠ¡å™¨æ‰¿å—èƒ½åŠ›è°ƒæ•´\n3. **ç¼“å­˜æœºåˆ¶**ï¼šé¿å…é‡å¤è¯·æ±‚ç›¸åŒå†…å®¹\n4. **èµ„æºæ¸…ç†**ï¼šåŠæ—¶å…³é—­ä¼šè¯å’Œè¿æ¥\n\n### ğŸ”§ è°ƒè¯•æŠ€å·§\n\n```python\n# å¯ç”¨è¯¦ç»†æ—¥å¿—\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# ç›‘æ§æ€§èƒ½\nstart = time.time()\n# ... çˆ¬è™«ä»£ç  ...\nprint(f"è€—æ—¶: {time.time() - start:.2f}s")\n```\n\n## 8. å®Œæ•´ç¤ºä¾‹ï¼šæ–°é—»ç½‘ç«™çˆ¬å–\n\n```python\nasync def main_news_crawler():\n    """æ–°é—»ç½‘ç«™çˆ¬å–ç¤ºä¾‹"""\n    config = CrawlerConfig()\n    config.max_concurrent_requests = 3\n    config.headers.update({\n        \'User-Agent\': \'Mozilla/5.0 (compatible; NewsBot/1.0)\'\n    })\n    \n    news_urls = [\n        \'https://example-news.com/article/1\',\n        \'https://example-news.com/article/2\',\n        # ... æ›´å¤šURL\n    ]\n    \n    async with AsyncCrawler(config) as crawler:\n        results = await crawler.crawl_urls(news_urls)\n        \n        # å¯¼å‡ºæ•°æ®\n        exporter = DataExporter()\n        exporter.export_to_json(results, "news_data")\n        exporter.export_to_csv(results, "news_data")\n\n# è¿è¡Œ\nif __name__ == "__main__":\n    asyncio.run(main_news_crawler())\n```\n\n## æ€»ç»“\n\nå¼‚æ­¥çˆ¬è™«çš„ä¼˜åŠ¿ï¼š\n- âœ… é«˜æ•ˆåˆ©ç”¨ç½‘ç»œI/O\n- âœ… æ˜¾è‘—æå‡çˆ¬å–é€Ÿåº¦\n- âœ… èµ„æºåˆ©ç”¨ç‡é«˜\n\nä½¿ç”¨å»ºè®®ï¼š\n- éµå®ˆç½‘ç«™è§„åˆ™å’Œæ³•å¾‹æ³•è§„\n- åˆç†è®¾ç½®è¯·æ±‚é¢‘ç‡\n- åšå¥½é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶\n- ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ\n\né€šè¿‡åˆç†ä½¿ç”¨å¼‚æ­¥çˆ¬è™«ï¼Œå¯ä»¥é«˜æ•ˆåœ°å®Œæˆå¤§è§„æ¨¡æ•°æ®é‡‡é›†ä»»åŠ¡ï¼Œä½†åŠ¡å¿…æ³¨æ„åˆæ³•åˆè§„ä½¿ç”¨ã€‚'